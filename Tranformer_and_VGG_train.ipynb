{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116 images into 'data' list.\n",
      "Loaded 17 images into 'data' list.\n",
      "Loaded 33 images into 'data' list.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Base directory containing the dataset\n",
    "base_directory = \"Zebra_fish_data/Balanced_dataset/train\"\n",
    "\n",
    "# Subfolder names corresponding to labels\n",
    "subfolders = {\n",
    "    1: \"images_1\",\n",
    "    2: \"images_2\",\n",
    "    3: \"images_3\",\n",
    "    4: \"images_4\"\n",
    "}\n",
    "\n",
    "# Initialize lists for data and labels\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "# Load images and labels from each subfolder\n",
    "for label, folder_name in subfolders.items():\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load image and convert to numpy array\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Append image and label to respective lists\n",
    "        train_data.append(image_array)\n",
    "        train_labels.append(label)\n",
    "\n",
    "# Base directory containing the dataset\n",
    "base_directory = \"Zebra_fish_data/Balanced_dataset/val\"\n",
    "\n",
    "# Initialize lists for data and labels\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "# Load images and labels from each subfolder\n",
    "for label, folder_name in subfolders.items():\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load image and convert to numpy array\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Append image and label to respective lists\n",
    "        val_data.append(image_array)\n",
    "        val_labels.append(label)\n",
    "   \n",
    "   \n",
    "# Base directory containing the dataset\n",
    "base_directory = \"Zebra_fish_data/Balanced_dataset/test\"\n",
    "\n",
    "# Initialize lists for data and labels\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "# Load images and labels from each subfolder\n",
    "for label, folder_name in subfolders.items():\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load image and convert to numpy array\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Append image and label to respective lists\n",
    "        test_data.append(image_array)\n",
    "        test_labels.append(label)     \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Confirm loaded data\n",
    "print(f\"Loaded {len(train_data)} images into 'data' list.\")\n",
    "print(f\"Loaded {len(val_data)} images into 'data' list.\")\n",
    "print(f\"Loaded {len(test_data)} images into 'data' list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import timm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seed = 111\n",
    "\n",
    "# Configure the ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.20,\n",
    "    height_shift_range=0.20,\n",
    "    zoom_range=0.10,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "# Function to apply data augmentation to a batch\n",
    "def apply_data_augmentation(batch_data):\n",
    "    batch_data_array = np.array(batch_data)\n",
    "    augmented_batch_data = []\n",
    "    for image in batch_data_array:\n",
    "        augmented_image = train_datagen.random_transform(image)\n",
    "        augmented_batch_data.append(augmented_image)\n",
    "    return augmented_batch_data\n",
    "\n",
    "# Feature extractor function (you might want to adjust this based on the specific model you are using)\n",
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/deit-small-distilled-patch16-224\")\n",
    "\n",
    "\"\"\"\n",
    "Train Val Test data is already loadad\n",
    "# Split the data into 80% learning and 20% test data\n",
    "X_learn, test_data, y_learn, test_labels = train_test_split(\n",
    "    data,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Split the learning data into training and validation sets (90% train, 10% validation)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    X_learn,\n",
    "    y_learn,\n",
    "    test_size=0.1,\n",
    "    stratify=y_learn,\n",
    "    random_state=seed\n",
    ")\"\"\"\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=25, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(val_data, val_labels)), batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_data, test_labels)), batch_size=25, shuffle=True)\n",
    "\n",
    "# Load the pre-trained DeiT model with 224x224 resolution\n",
    "model = timm.create_model('deit_small_distilled_patch16_224', pretrained=True)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Lists to track metrics\n",
    "training_loss_list = []\n",
    "training_acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Acc: 0.1724137931034483\n",
      "Validation Accuracy: 41.18%\n",
      "Model saved to Models/Transformer\\trans_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')  # Print the current epoch number\n",
    "    \n",
    "    # Initialize metrics for training\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = apply_data_augmentation(inputs)  # Apply data augmentation to the input batch\n",
    "        optimizer.zero_grad()  # Reset gradients from the previous step\n",
    "        \n",
    "        # Preprocess inputs and perform forward pass\n",
    "        batch = feature_extractor(list(inputs), return_tensors=\"pt\", do_normalize=True)\n",
    "        pixel_values = batch['pixel_values']  # Extract the tensor for the model input\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "        # Calculate loss and perform backward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model weights\n",
    "        \n",
    "        # Accumulate loss and accuracy metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Store training metrics for later analysis\n",
    "    training_loss_list.append(epoch_loss)\n",
    "    training_acc_list.append(epoch_accuracy)\n",
    "\n",
    "    print(f\"Train Acc: {epoch_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for inputs, labels in val_loader:\n",
    "            batch = feature_extractor(list(inputs), return_tensors=\"pt\", do_normalize=True)\n",
    "            pixel_values = batch['pixel_values']\n",
    "            outputs = model(pixel_values)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate validation accuracy for the epoch\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "\n",
    "# Log metrics and test labels\n",
    "log_directory = f\"Models/Transformer\"\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Save the model at the end of training (after the final epoch)\n",
    "final_model_path = os.path.join(log_directory, 'trans_model.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f'Model saved to {final_model_path}')\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(log_directory, 'trans_metrics.txt')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    f.write('Training Loss List:\\n')\n",
    "    f.write(str(training_loss_list) + '\\n\\n')\n",
    "    f.write('Training Accuracy List:\\n')\n",
    "    f.write(str(training_acc_list) + '\\n\\n')\n",
    "    f.write('Validation Accuracy List:\\n')\n",
    "    f.write(str(val_acc_list) + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 640, 640, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 640, 640, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 640, 640, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 320, 320, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 320, 320, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 320, 320, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 160, 160, 128)     0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 160, 160, 256)     295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 160, 160, 256)     590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 160, 160, 256)     590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 80, 80, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 80, 80, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 80, 80, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 80, 80, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 40, 40, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 40, 40, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 40, 40, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 40, 40, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 20, 20, 512)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 204800)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               104858112 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,573,313\n",
      "Trainable params: 104,858,625\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "X_train shape: (116, 640, 640, 3), y_train shape: (116,)\n",
      "X_val shape: (17, 640, 640, 3), y_val shape: (17,)\n",
      "X_test shape: (33, 640, 640, 3), y_test shape: (33,)\n",
      "Epoch 1/45\n",
      "4/4 [==============================] - 107s 25s/step - loss: 240.8016 - accuracy: 0.2241 - val_loss: 187.1611 - val_accuracy: 0.3529\n",
      "Epoch 2/45\n",
      "4/4 [==============================] - 115s 30s/step - loss: 147.6525 - accuracy: 0.2586 - val_loss: 104.8203 - val_accuracy: 0.3529\n",
      "Epoch 3/45\n",
      "4/4 [==============================] - 150s 43s/step - loss: 59.1889 - accuracy: 0.2586 - val_loss: 24.7409 - val_accuracy: 0.3529\n",
      "Epoch 4/45\n",
      "4/4 [==============================] - 153s 38s/step - loss: -18.0627 - accuracy: 0.2586 - val_loss: -54.5694 - val_accuracy: 0.3529\n",
      "Epoch 5/45\n",
      "4/4 [==============================] - 156s 39s/step - loss: -97.6132 - accuracy: 0.2586 - val_loss: -132.7978 - val_accuracy: 0.3529\n",
      "Epoch 6/45\n",
      "4/4 [==============================] - 157s 39s/step - loss: -185.3857 - accuracy: 0.2586 - val_loss: -209.8554 - val_accuracy: 0.3529\n",
      "Epoch 7/45\n",
      "4/4 [==============================] - 155s 39s/step - loss: -261.8982 - accuracy: 0.2586 - val_loss: -286.0917 - val_accuracy: 0.3529\n",
      "Epoch 8/45\n",
      "4/4 [==============================] - 155s 38s/step - loss: -343.5369 - accuracy: 0.2586 - val_loss: -362.9120 - val_accuracy: 0.3529\n",
      "Epoch 9/45\n",
      "4/4 [==============================] - 156s 44s/step - loss: -440.6920 - accuracy: 0.2586 - val_loss: -441.7473 - val_accuracy: 0.3529\n",
      "Epoch 10/45\n",
      "4/4 [==============================] - 155s 38s/step - loss: -514.4198 - accuracy: 0.2586 - val_loss: -523.1467 - val_accuracy: 0.3529\n",
      "Epoch 11/45\n",
      "4/4 [==============================] - 156s 39s/step - loss: -605.7728 - accuracy: 0.2586 - val_loss: -605.9822 - val_accuracy: 0.3529\n",
      "Epoch 12/45\n",
      "4/4 [==============================] - 155s 43s/step - loss: -707.1649 - accuracy: 0.2586 - val_loss: -693.0873 - val_accuracy: 0.3529\n",
      "Epoch 13/45\n",
      "4/4 [==============================] - 155s 38s/step - loss: -771.7516 - accuracy: 0.2586 - val_loss: -790.8267 - val_accuracy: 0.3529\n",
      "Epoch 14/45\n",
      "4/4 [==============================] - 152s 38s/step - loss: -908.1172 - accuracy: 0.2586 - val_loss: -900.4694 - val_accuracy: 0.3529\n",
      "Epoch 15/45\n",
      "4/4 [==============================] - 156s 43s/step - loss: -1043.6724 - accuracy: 0.2586 - val_loss: -1022.2458 - val_accuracy: 0.3529\n",
      "Epoch 16/45\n",
      "4/4 [==============================] - 156s 39s/step - loss: -1149.2283 - accuracy: 0.2586 - val_loss: -1147.8309 - val_accuracy: 0.3529\n",
      "Epoch 17/45\n",
      "4/4 [==============================] - 155s 43s/step - loss: -1297.9374 - accuracy: 0.2586 - val_loss: -1276.9277 - val_accuracy: 0.3529\n",
      "Epoch 18/45\n",
      "4/4 [==============================] - 153s 42s/step - loss: -1448.3289 - accuracy: 0.2586 - val_loss: -1407.1050 - val_accuracy: 0.3529\n",
      "Epoch 19/45\n",
      "4/4 [==============================] - 154s 38s/step - loss: -1570.9435 - accuracy: 0.2586 - val_loss: -1540.5924 - val_accuracy: 0.3529\n",
      "Epoch 20/45\n",
      "4/4 [==============================] - 154s 39s/step - loss: -1729.9792 - accuracy: 0.2586 - val_loss: -1676.1805 - val_accuracy: 0.3529\n",
      "Epoch 21/45\n",
      "4/4 [==============================] - 4065s 1342s/step - loss: -1888.7197 - accuracy: 0.2586 - val_loss: -1812.7330 - val_accuracy: 0.3529\n",
      "Epoch 22/45\n",
      "4/4 [==============================] - 84s 23s/step - loss: -2001.6628 - accuracy: 0.2586 - val_loss: -1953.5687 - val_accuracy: 0.3529\n",
      "Epoch 23/45\n",
      "4/4 [==============================] - 84s 21s/step - loss: -2143.9351 - accuracy: 0.2586 - val_loss: -2094.7903 - val_accuracy: 0.3529\n",
      "Epoch 24/45\n",
      "4/4 [==============================] - 84s 21s/step - loss: -2332.6487 - accuracy: 0.2586 - val_loss: -2236.8098 - val_accuracy: 0.3529\n",
      "Epoch 25/45\n",
      "4/4 [==============================] - 97s 25s/step - loss: -2510.6821 - accuracy: 0.2586 - val_loss: -2381.4995 - val_accuracy: 0.3529\n",
      "Epoch 26/45\n",
      "4/4 [==============================] - 140s 36s/step - loss: -2646.9673 - accuracy: 0.2586 - val_loss: -2528.7258 - val_accuracy: 0.3529\n",
      "Epoch 27/45\n",
      "4/4 [==============================] - 144s 35s/step - loss: -2821.4241 - accuracy: 0.2586 - val_loss: -2679.5649 - val_accuracy: 0.3529\n",
      "Epoch 28/45\n",
      "4/4 [==============================] - 2801s 922s/step - loss: -2982.4370 - accuracy: 0.2586 - val_loss: -2833.0264 - val_accuracy: 0.3529\n",
      "Epoch 29/45\n",
      "4/4 [==============================] - 84s 23s/step - loss: -3130.5061 - accuracy: 0.2586 - val_loss: -2988.4915 - val_accuracy: 0.3529\n",
      "Epoch 30/45\n",
      "4/4 [==============================] - 85s 23s/step - loss: -3279.4382 - accuracy: 0.2586 - val_loss: -3144.0874 - val_accuracy: 0.3529\n",
      "Epoch 31/45\n",
      "4/4 [==============================] - 87s 22s/step - loss: -3510.1582 - accuracy: 0.2586 - val_loss: -3304.2493 - val_accuracy: 0.3529\n",
      "Epoch 32/45\n",
      "4/4 [==============================] - 136s 33s/step - loss: -3652.5547 - accuracy: 0.2586 - val_loss: -3465.3748 - val_accuracy: 0.3529\n",
      "Epoch 33/45\n",
      "4/4 [==============================] - 97s 25s/step - loss: -3802.6946 - accuracy: 0.2586 - val_loss: -3630.1526 - val_accuracy: 0.3529\n",
      "Epoch 34/45\n",
      "4/4 [==============================] - 135s 33s/step - loss: -3997.6621 - accuracy: 0.2586 - val_loss: -3795.5449 - val_accuracy: 0.3529\n",
      "Epoch 35/45\n",
      "4/4 [==============================] - 134s 33s/step - loss: -4204.5576 - accuracy: 0.2586 - val_loss: -3964.7568 - val_accuracy: 0.3529\n",
      "Epoch 36/45\n",
      "4/4 [==============================] - 135s 33s/step - loss: -4361.2275 - accuracy: 0.2586 - val_loss: -4136.5601 - val_accuracy: 0.3529\n",
      "Epoch 37/45\n",
      "4/4 [==============================] - 137s 34s/step - loss: -4578.4634 - accuracy: 0.2586 - val_loss: -4311.6548 - val_accuracy: 0.3529\n",
      "Epoch 38/45\n",
      "4/4 [==============================] - 135s 33s/step - loss: -4670.0796 - accuracy: 0.2586 - val_loss: -4489.8672 - val_accuracy: 0.3529\n",
      "Epoch 39/45\n",
      "4/4 [==============================] - 134s 33s/step - loss: -5053.5303 - accuracy: 0.2586 - val_loss: -4667.6543 - val_accuracy: 0.3529\n",
      "Epoch 40/45\n",
      "4/4 [==============================] - 134s 37s/step - loss: -5231.1426 - accuracy: 0.2586 - val_loss: -4851.1328 - val_accuracy: 0.3529\n",
      "Epoch 41/45\n",
      "4/4 [==============================] - 134s 37s/step - loss: -5305.7026 - accuracy: 0.2586 - val_loss: -5036.8608 - val_accuracy: 0.3529\n",
      "Epoch 42/45\n",
      "4/4 [==============================] - 133s 33s/step - loss: -5612.5449 - accuracy: 0.2586 - val_loss: -5226.5532 - val_accuracy: 0.3529\n",
      "Epoch 43/45\n",
      "4/4 [==============================] - 137s 38s/step - loss: -5774.4873 - accuracy: 0.2586 - val_loss: -5414.6738 - val_accuracy: 0.3529\n",
      "Epoch 44/45\n",
      "4/4 [==============================] - 136s 34s/step - loss: -5906.2847 - accuracy: 0.2586 - val_loss: -5608.3906 - val_accuracy: 0.3529\n",
      "Epoch 45/45\n",
      "4/4 [==============================] - 136s 34s/step - loss: -6184.5947 - accuracy: 0.2586 - val_loss: -5803.0000 - val_accuracy: 0.3529\n",
      "Metrics saved to Models/VGG16\\vgg_16_metrics.json\n",
      "Model saved to Models/VGG16\\vgg_16_model.h5\n",
      "2/2 [==============================] - 41s 1s/step - loss: -3642.8181 - accuracy: 0.1818\n",
      "Test Accuracy: 0.1818181872367859\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Data augmentation function\n",
    "def preprocess_function(image):\n",
    "    # Adjust contrast of the image\n",
    "    image = tf.image.adjust_contrast(image, 1.2)\n",
    "    return image\n",
    "\n",
    "# ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    rotation_range=20,  # Randomly rotate images\n",
    "    zoom_range=0.2,  # Randomly zoom images\n",
    "    fill_mode='nearest',  # Fill in missing pixels after transformations\n",
    "    preprocessing_function=preprocess_function  # Custom preprocessing function\n",
    ")\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "# Include only convolutional base (no fully connected layers) and use the specified input shape\n",
    "input_shape = (640, 640, 3)  # Specify the input shape (640x640 with 3 color channels)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze all layers in the pre-trained model\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = Flatten()(vgg16.output)  # Flatten the feature map into a 1D vector\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l1(0.001))(x)  # Fully connected layer with L1 regularization\n",
    "x = Dropout(0.5)(x)  # Dropout to prevent overfitting\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=vgg16.input, outputs=predictions)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Normalize the data and convert to NumPy arrays\n",
    "X_train = np.array([np.array(image) for image in train_data], dtype=np.float32) / 255.0\n",
    "X_val = np.array([np.array(image) for image in val_data], dtype=np.float32) / 255.0\n",
    "X_test = np.array([np.array(image) for image in test_data], dtype=np.float32) / 255.0\n",
    "\n",
    "# Preprocess input data for VGG16 (standardize based on ImageNet)\n",
    "X_train = preprocess_input(X_train)\n",
    "X_val = preprocess_input(X_val)\n",
    "#X_test = preprocess_input(X_test)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Create a training data generator with data augmentation\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Optimizer with specified learning rate\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,  # Use the augmented data generator for training\n",
    "    epochs=num_epochs,  # Train for 45 epochs (adjust as needed)\n",
    "    validation_data=(X_val, y_val)  # Use the validation set for evaluation\n",
    ")\n",
    "\n",
    "# Save metrics after training\n",
    "save_directory = f\"Models/VGG16\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "metrics = {\n",
    "    \"train_loss\": history.history['loss'],  # Training loss for each epoch\n",
    "    \"train_accuracy\": history.history['accuracy'],  # Training accuracy for each epoch\n",
    "    \"val_accuracy\": history.history['val_accuracy'],  # Validation accuracy for each epoch\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(save_directory, \"vgg_16_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(save_directory, \"vgg_16_model.h5\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
