{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 166 images into 'data' list.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Base directory containing the dataset\n",
    "base_directory = \"Zebra_fish_data/Balanced_dataset\"\n",
    "\n",
    "# Subfolder names corresponding to labels\n",
    "subfolders = {\n",
    "    1: \"images_1\",\n",
    "    2: \"images_2\",\n",
    "    3: \"images_3\",\n",
    "    4: \"images_4\"\n",
    "}\n",
    "\n",
    "# Initialize lists for data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load images and labels from each subfolder\n",
    "for label, folder_name in subfolders.items():\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load image and convert to numpy array\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Append image and label to respective lists\n",
    "        data.append(image_array)\n",
    "        labels.append(label)\n",
    "\n",
    "# Confirm loaded data\n",
    "print(f\"Loaded {len(data)} images into 'data' list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_data = [Image.fromarray(image).resize((224, 224)) if isinstance(image, np.ndarray) else image.resize((224, 224)) for image in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import timm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seed = 111\n",
    "\n",
    "# Configure the ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.20,\n",
    "    height_shift_range=0.20,\n",
    "    zoom_range=0.10,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "# Function to apply data augmentation to a batch\n",
    "def apply_data_augmentation(batch_data):\n",
    "    batch_data_array = np.array(batch_data)\n",
    "    augmented_batch_data = []\n",
    "    for image in batch_data_array:\n",
    "        augmented_image = train_datagen.random_transform(image)\n",
    "        augmented_batch_data.append(augmented_image)\n",
    "    return augmented_batch_data\n",
    "\n",
    "# Feature extractor function (you might want to adjust this based on the specific model you are using)\n",
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/deit-small-distilled-patch16-224\")\n",
    "\n",
    "# Split the data into 80% learning and 20% test data\n",
    "X_learn, test_data, y_learn, test_labels = train_test_split(\n",
    "    data,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Split the learning data into training and validation sets (90% train, 10% validation)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    X_learn,\n",
    "    y_learn,\n",
    "    test_size=0.1,\n",
    "    stratify=y_learn,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=25, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(val_data, val_labels)), batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_data, test_labels)), batch_size=25, shuffle=True)\n",
    "\n",
    "# Load the pre-trained DeiT model with 224x224 resolution\n",
    "model = timm.create_model('deit_small_distilled_patch16_224', pretrained=True)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 45\n",
    "\n",
    "# Lists to track metrics\n",
    "training_loss_list = []\n",
    "training_acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "Train Acc: 0.1864406779661017\n",
      "Validation Accuracy: 35.71%\n",
      "Epoch 2/45\n",
      "Train Acc: 0.4491525423728814\n",
      "Validation Accuracy: 57.14%\n",
      "Epoch 3/45\n",
      "Train Acc: 0.5254237288135594\n",
      "Validation Accuracy: 64.29%\n",
      "Epoch 4/45\n",
      "Train Acc: 0.6186440677966102\n",
      "Validation Accuracy: 71.43%\n",
      "Epoch 5/45\n",
      "Train Acc: 0.711864406779661\n",
      "Validation Accuracy: 42.86%\n",
      "Epoch 6/45\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')  # Print the current epoch number\n",
    "    \n",
    "    # Initialize metrics for training\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = apply_data_augmentation(inputs)  # Apply data augmentation to the input batch\n",
    "        optimizer.zero_grad()  # Reset gradients from the previous step\n",
    "        \n",
    "        # Preprocess inputs and perform forward pass\n",
    "        batch = feature_extractor(list(inputs), return_tensors=\"pt\", do_normalize=True)\n",
    "        pixel_values = batch['pixel_values']  # Extract the tensor for the model input\n",
    "\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "        # Calculate loss and perform backward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model weights\n",
    "        \n",
    "        # Accumulate loss and accuracy metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Store training metrics for later analysis\n",
    "    training_loss_list.append(epoch_loss)\n",
    "    training_acc_list.append(epoch_accuracy)\n",
    "\n",
    "    print(f\"Train Acc: {epoch_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for inputs, labels in val_loader:\n",
    "            batch = feature_extractor(list(inputs), return_tensors=\"pt\", do_normalize=True)\n",
    "            pixel_values = batch['pixel_values']\n",
    "            outputs = model(pixel_values)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate validation accuracy for the epoch\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "\n",
    "# Log metrics and test labels\n",
    "log_directory = f\"Models/Transformer\"\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Save the model at the end of training (after the final epoch)\n",
    "final_model_path = os.path.join(log_directory, 'trans_model.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f'Model saved to {final_model_path}')\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(log_directory, 'trans_metrics.txt')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    f.write('Training Loss List:\\n')\n",
    "    f.write(str(training_loss_list) + '\\n\\n')\n",
    "    f.write('Training Accuracy List:\\n')\n",
    "    f.write(str(training_acc_list) + '\\n\\n')\n",
    "    f.write('Validation Accuracy List:\\n')\n",
    "    f.write(str(val_acc_list) + '\\n\\n')\n",
    "\n",
    "# Save test labels\n",
    "test_labels_path = os.path.join(log_directory, 'test_labels.txt')\n",
    "test_indices = test_labels.index.tolist()\n",
    "with open(test_labels_path, 'w') as f:\n",
    "    f.write('Test Labels with Row Indices:\\n')\n",
    "    for index, label in zip(test_indices, test_labels):\n",
    "        f.write(f'Row {index}: Label {label}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,560,769\n",
      "Trainable params: 12,846,081\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "X_train shape: (118, 640, 640, 3), y_train shape: (118,)\n",
      "X_val shape: (14, 640, 640, 3), y_val shape: (14,)\n",
      "X_test shape: (34, 640, 640, 3), y_test shape: (34,)\n",
      "Epoch 1/45\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Input to reshape is a tensor with 6553600 values, but the requested shape requires a multiple of 25088\n\t [[node model/flatten/Reshape\n (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py:96)\n]] [Op:__inference_train_function_1575]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/flatten/Reshape:\nIn[0] model/block5_pool/MaxPool (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py:357)\t\nIn[1] model/flatten/Const (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py:91)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-3-845cc13dba4d>\", line 71, in <module>\n>>>     history = model.fit(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py\", line 96, in call\n>>>     return tf.reshape(inputs, flattened_shape)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-845cc13dba4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Use the augmented data generator for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Train for 45 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 6553600 values, but the requested shape requires a multiple of 25088\n\t [[node model/flatten/Reshape\n (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py:96)\n]] [Op:__inference_train_function_1575]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/flatten/Reshape:\nIn[0] model/block5_pool/MaxPool (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py:357)\t\nIn[1] model/flatten/Const (defined at c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py:91)\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-3-845cc13dba4d>\", line 71, in <module>\n>>>     history = model.fit(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"c:\\Users\\9andy\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\flatten.py\", line 96, in call\n>>>     return tf.reshape(inputs, flattened_shape)\n>>> "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Data augmentation function\n",
    "def preprocess_function(image):\n",
    "    # Adjust contrast of the image\n",
    "    image = tf.image.adjust_contrast(image, 1.2)\n",
    "    return image\n",
    "\n",
    "# ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    rotation_range=20,  # Randomly rotate images\n",
    "    zoom_range=0.2,  # Randomly zoom images\n",
    "    fill_mode='nearest',  # Fill in missing pixels after transformations\n",
    "    preprocessing_function=preprocess_function  # Custom preprocessing function\n",
    ")\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "# Include only convolutional base (no fully connected layers) and use the specified input shape\n",
    "input_shape = (224, 224, 3)  # Specify the input shape (224x224 with 3 color channels)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze all layers in the pre-trained model\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = Flatten()(vgg16.output)  # Flatten the feature map into a 1D vector\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l1(0.001))(x)  # Fully connected layer with L1 regularization\n",
    "x = Dropout(0.5)(x)  # Dropout to prevent overfitting\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=vgg16.input, outputs=predictions)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Normalize the data and convert to NumPy arrays\n",
    "X_train = np.array([np.array(image) for image in train_data], dtype=np.float32) / 255.0\n",
    "X_val = np.array([np.array(image) for image in val_data], dtype=np.float32) / 255.0\n",
    "X_test = np.array([np.array(image) for image in test_data], dtype=np.float32) / 255.0\n",
    "\n",
    "# Preprocess input data for VGG16 (standardize based on ImageNet)\n",
    "X_train = preprocess_input(X_train)\n",
    "X_val = preprocess_input(X_val)\n",
    "X_test = preprocess_input(X_test)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Create a training data generator with data augmentation\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Optimizer with specified learning rate\n",
    "    loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,  # Use the augmented data generator for training\n",
    "    epochs=num_epochs,  # Train for 45 epochs (adjust as needed)\n",
    "    validation_data=(X_val, y_val)  # Use the validation set for evaluation\n",
    ")\n",
    "\n",
    "# Save metrics after training\n",
    "save_directory = f\"Models/VGG16\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "metrics = {\n",
    "    \"train_loss\": history.history['loss'],  # Training loss for each epoch\n",
    "    \"train_accuracy\": history.history['accuracy'],  # Training accuracy for each epoch\n",
    "    \"val_accuracy\": history.history['val_accuracy'],  # Validation accuracy for each epoch\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(save_directory, \"vgg_16_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(save_directory, \"vgg_16_model.h5\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
